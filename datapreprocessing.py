# -*- coding: utf-8 -*-
"""DataPreprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jru2F7ytha8T2V5Cw63KxXOedP6gROnf
"""

# importing the dependencies
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

#load libraries
import numpy as np 
import pandas as pd

from scipy import stats
from scipy.stats import norm

from google.colab import drive
drive.mount('/content/drive')

#loading training dataset
df = pd.read_csv('/content/drive/MyDrive/train.csv') # changed path as present in my drive

df.head()

# splitting the dataset into 2 subsequent to preprocess easily

million = 10**6
first_df = df.iloc[:million+1]
second_df = df.iloc[million+1:]

# Preprocessing dataset-1
df1 = first_df

# removing null values

df1['TITLE'] = df1['TITLE'].fillna('')
df1['BULLET_POINTS'] = df1['BULLET_POINTS'].fillna('')
df1['DESCRIPTION'] = df1['DESCRIPTION'].fillna('')

# appending title, bullet points and description into a single column

df1['data'] = df1['TITLE'] + " " + df1['BULLET_POINTS']    + " " + df1['DESCRIPTION']

# removing unneccessary columns

del df1['BULLET_POINTS']
del df1['DESCRIPTION']
del df1['TITLE']

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Get a list of adjectives
adjectives = set()
for synset in nltk.corpus.wordnet.all_synsets():
    for lemma in synset.lemmas():
        if lemma.synset().pos() == 'a':
            adjectives.add(lemma.name().lower())

unwanted_words = ['big', 'small', 'large', 'long', 'wide', 'tall', 'short', 'deep', 'thick', 'narrow', 'broad', 'high', 'low', 'thinner', 'thicker', 'wider', 'narrower', 'higher', 'lower', 'taller', 'shorter', 'deeper', 'shallower']+['Long','Short','Tall','Wide','Narrow','Broad','Deep','Shallow','Thick','Thin','Fat','Slim','Compact','Roomy','Spacious','High','Low','Big','Small','Large','Miniature','Petite','Jumbo','Extra','large','Tiny','Minuscule','Giant','Enormous','Little','Huge','Thin','Fine','Coarse','Chunky','Bulky']+['Long','Short','Tall','Wide','Narrow','Broad','Deep','Shallow','Thick','Thin','Fat','Slim','Compact','Roomy','Spacious','High','Low','Big','Small','Large','Miniature','Petite','Jumbo','Extra','large','Tiny','Minuscule','Giant','Enormous','Little','Huge','Thin','Fine','Coarse','Chunky','Bulky']

adjectives = adjectives - set(unwanted_words)

# Preparing a set of stop words to omit

stop_words = set(stopwords.words('english'))
for i in adjectives:
  stop_words.add(i)

# removing html tags 

def remove_html_tags(text):
  return re.sub(r'<.*?>', '', text)

# removing punctuations 

def remove_punctuations(text):
  punctuations = string.punctuation

  return text.translate(str.maketrans('', '', punctuations))

# removing stopwords

def remove_stopwords(text):
  return ' '.join([word for word in text.split() if word not in stop_words])

# Lemmatizing the textual data (reducing word to its root word)
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    return " ".join([lemmatizer.lemmatize(word) for word in set(text.split())])

# converting text to lower case
df1['data'] = df1['data'].str.lower()

# applying operations on dataset

df1['data'] = df1['data'].apply(lambda x: remove_html_tags(x))
df1['data'] = df1['data'].apply(lambda x: remove_punctuations(x))
df1['data'] = df1['data'].apply(lambda x: remove_stopwords(x))
df1['data'] = df1['data'].apply(lambda x: lemmatize_text(x))

#log tranform helps to tranform our data as close to normal skewness. print ('Skewness is', df['PRODUCT_LENGTH'].skew())
df1['PRODUCT_LENGTH'] = np.log(df1['PRODUCT_LENGTH'])

# Preprocessed data of dataset-1
df1.head()

# Preprocessing on dataset-2
df2 = second_df

# removing null values

df2['TITLE'] = df2['TITLE'].fillna('')
df2['BULLET_POINTS'] = df2['BULLET_POINTS'].fillna('')
df2['DESCRIPTION'] = df2['DESCRIPTION'].fillna('')

# appending title, bulletpoints, description to data column

df2['data'] = df2['TITLE'] + " " + df2['BULLET_POINTS'] + " " + df2['DESCRIPTION']

# deleting unneccessay columns

del df2['BULLET_POINTS']
del df2['DESCRIPTION']
del df2['TITLE']

# to lowercase
df2['data'] = df2['data'].str.lower()

# operations on dataset-2

df2['data'] = df2['data'].apply(lambda x: remove_html_tags(x))
df2['data'] = df2['data'].apply(lambda x: remove_punctuations(x))
df2['data'] = df2['data'].apply(lambda x: remove_stopwords(x))
df2['data'] = df2['data'].apply(lambda x: lemmatize_text(x))

#Preprocessed data of dataset-2
df2.head()

# Merging both datasets
merged_df = pd.DataFrame([df1, df2])

# The final Preprocessed dataset
merged_df.to_csv('preprocessed_data.csv', index=False)

"""Testing dataset"""

# Preprocessing on Testing data

# loading testing dataset
test_df = pd.read_csv('/content/drive/MyDrive/test.csv', usecols=['PRODUCT_ID', 'TITLE', 'BULLET_POINTS', 'DESCRIPTION'])

test_df.head()

# removing null values
test_df = test_df.fillna('')

# appending title bulletpoints description to content column
test_df['CONTENT'] = test_df['TITLE'] + ' ' + test_df['BULLET_POINTS'] + ' ' + test_df['DESCRIPTION']

# delete unneccessary columns
del test_df['TITLE']
del test_df['BULLET_POINTS']
del test_df['DESCRIPTION']

# converting text to lowercase
test_df['CONTENT'] = test_df['CONTENT'].str.lower()

test_df['CONTENT'] = test_df['CONTENT'].apply(lambda x: remove_html_tags(x))
test_df['CONTENT'] = test_df['CONTENT'].apply(lambda x: remove_punctuations(x))
test_df['CONTENT'] = test_df['CONTENT'].apply(lambda x: remove_stopwords(x))
test_df['CONTENT'] = test_df['CONTENT'].apply(lambda x: lemmatize_text(x))

test_df.head()

test_df.to_csv('preprocessed_test_data.csv', index=False)